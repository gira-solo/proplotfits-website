---
title: "A Tale of Two Hawks Part 3"
description: "How to train, validate and deploy predictive models"
date: last-modified
categories: [college-basketball, machine-learning, tutorial, kansas, miami-oh]
author: "ProPlotFits"
---

## Introduction

Why am I trying to see who would win in a college basketball team between Kansas of the Big 12 and Miami of the MAC? These teams haven't played each other since 2011. All of those answers can be answered in Part 1 and Part 2.

Here, the goal is to find out what would happen if Kansas and Miami were to play each other.

---

## The Three Prediction Tasks

We have three outcomes that require three models:

**Model 1: Binary Classification** - Will one specific team win?  
**Model 2: Continuous Regression (Difference)** - By how much might the score differ between teams?  
**Model 3: Continuous Regression (Sum)** - What might the combined scoring output look like?

Each model uses the same underlying features but optimizes for different objectives. Together they give us a complete picture of what might happen.

---

## Step 1: Prepare Historical Data

We need clean, structured data where each row represents one game from the perspective of both teams.

```{r}
#| warning: false
#| message: false

library(tidyverse)
library(hoopR)
library(janitor)
library(caret)

# Load historical data (2024 and 2025 seasons)
box_scores_2024 <- load_mbb_team_box(seasons = 2024) %>% clean_names()
box_scores_2025 <- load_mbb_team_box(seasons = 2025) %>% clean_names()

# Combine seasons
all_games <- bind_rows(
  box_scores_2024 %>% mutate(season = 2024),
  box_scores_2025 %>% mutate(season = 2025)
)

glimpse(all_games)
```

---

## Step 2: Feature Engineering

We calculate team-level statistics that capture performance quality.

```{r}
#| code-fold: true

# Calculate four factors and efficiency metrics
team_features <- all_games %>%
  mutate(
    # Four Factors
    efg_pct = (field_goals_made + 0.5 * three_point_field_goals_made) / 
              field_goals_attempted,
    tov_pct = turnovers / 
              (field_goals_attempted + 0.44 * free_throws_attempted + turnovers),
    orb_pct = offensive_rebounds / 
              (offensive_rebounds + defensive_rebounds),
    ft_rate = free_throws_made / field_goals_attempted,
    
    # Efficiency
    possessions = field_goals_attempted - offensive_rebounds + 
                  turnovers + 0.44 * free_throws_attempted,
    ortg = (team_score / possessions) * 100,
    drtg = (opponent_team_score / possessions) * 100,
    net_rating = ortg - drtg,
    pace = possessions
  ) %>%
  select(
    season, game_date, game_id,
    team_display_name, team_id, team_home_away,
    team_score, opponent_team_score,
    efg_pct, tov_pct, orb_pct, ft_rate,
    ortg, drtg, net_rating, pace
  )

# Calculate rolling averages (last 5 games)
team_rolling <- team_features %>%
  arrange(team_id, game_date) %>%
  group_by(team_id) %>%
  mutate(
    efg_L5 = lag(zoo::rollmean(efg_pct, k = 5, fill = NA, align = "right")),
    tov_L5 = lag(zoo::rollmean(tov_pct, k = 5, fill = NA, align = "right")),
    orb_L5 = lag(zoo::rollmean(orb_pct, k = 5, fill = NA, align = "right")),
    ftr_L5 = lag(zoo::rollmean(ft_rate, k = 5, fill = NA, align = "right")),
    ortg_L5 = lag(zoo::rollmean(ortg, k = 5, fill = NA, align = "right")),
    drtg_L5 = lag(zoo::rollmean(drtg, k = 5, fill = NA, align = "right")),
    net_L5 = lag(zoo::rollmean(net_rating, k = 5, fill = NA, align = "right")),
    pace_L5 = lag(zoo::rollmean(pace, k = 5, fill = NA, align = "right"))
  ) %>%
  ungroup() %>%
  drop_na(efg_L5)  # Only keep games where we have rolling history

head(team_rolling)
```

**Key concept:** We use lagged rolling averages so we only use information available *before* each game. This prevents data leakage.

---

## Step 3: Adjust Data Granularity

Models predict games, not individual team performances. We need to pivot from team-game level to game level.

```{r}
#| code-fold: true

# Separate home and away
home_games <- team_rolling %>%
  filter(team_home_away == "home") %>%
  select(
    game_id, season, game_date,
    home_team = team_display_name,
    home_score = team_score,
    home_efg = efg_L5, home_tov = tov_L5, home_orb = orb_L5, home_ftr = ftr_L5,
    home_ortg = ortg_L5, home_drtg = drtg_L5, home_net = net_L5, home_pace = pace_L5
  )

away_games <- team_rolling %>%
  filter(team_home_away == "away") %>%
  select(
    game_id,
    away_team = team_display_name,
    away_score = team_score,
    away_efg = efg_L5, away_tov = tov_L5, away_orb = orb_L5, away_ftr = ftr_L5,
    away_ortg = ortg_L5, away_drtg = drtg_L5, away_net = net_L5, away_pace = pace_L5
  )

# Join and calculate outcomes
games <- home_games %>%
  inner_join(away_games, by = "game_id") %>%
  mutate(
    # Outcome variables
    home_win = as.factor(if_else(home_score > away_score, "Yes", "No")),
    score_diff = home_score - away_score,  # positive = home won
    total_score = home_score + away_score,
    
    # Matchup differentials
    efg_diff = home_efg - away_efg,
    tov_diff = home_tov - away_tov,
    orb_diff = home_orb - away_orb,
    ftr_diff = home_ftr - away_ftr,
    ortg_diff = home_ortg - away_ortg,
    drtg_diff = home_drtg - away_drtg,
    net_diff = home_net - away_net,
    pace_avg = (home_pace + away_pace) / 2
  ) %>%
  drop_na()

glimpse(games)
```

Now each row is one game with features for both teams and three outcome variables.

---

## Step 4: Train-Test Split

We use 2024 season for training and 2025 season for testing. This simulates real-world usage: train on history, predict the future.

```{r}
train_data <- games %>% filter(season == 2024)
test_data <- games %>% filter(season == 2025)

cat("Training games:", nrow(train_data), "\n")
cat("Testing games:", nrow(test_data), "\n")
```

---

## Model 1: Binary Outcome Prediction

**Goal:** Estimate probability that the home team wins

**Algorithm:** Logistic Regression

```{r}
#| warning: false

# Select features for classification
features_binary <- c(
  "efg_diff", "tov_diff", "orb_diff", "ftr_diff",
  "net_diff", "pace_avg"
)

# Prepare data
train_binary <- train_data %>%
  select(home_win, all_of(features_binary)) %>%
  drop_na()

test_binary <- test_data %>%
  select(home_win, all_of(features_binary)) %>%
  drop_na()

# Train model
set.seed(42)
model_binary <- train(
  home_win ~ .,
  data = train_binary,
  method = "glm",
  family = "binomial",
  trControl = trainControl(
    method = "cv",
    number = 5,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  ),
  metric = "ROC"
)

# Evaluate
pred_binary <- predict(model_binary, test_binary, type = "prob")
test_binary$pred_prob <- pred_binary$Yes
test_binary$pred_class <- predict(model_binary, test_binary)

# Accuracy
accuracy <- mean(test_binary$pred_class == test_binary$home_win)
cat("Test Set Accuracy:", round(accuracy * 100, 1), "%\n")

# Confusion Matrix
confusionMatrix(test_binary$pred_class, test_binary$home_win)
```

**Interpretation:** I'm really happy about this output, because it should spur us to investigate our data more. 

The model predicted 1,315 false positives (it predicted 1,315 home wins that were actually losses) according to the [Confusion Matrix](https://www.youtube.com/watch?v=Kdsp6soqA7o). The model is likely over-relying on the home-court advantage, which can work slightly better than a coin flip but it is not nuanced enough to when the away team is actually better. 

Ultimately, if I were to develop a machine learning model to help me determine moneyline, I would go for one that outputs probabilities rather than straight-up Yes/No outcomes. 

---

## Model 2: Score Differential Prediction

**Goal:** What is the likely difference in final scores?

**Algorithm:** Gradient Boosting Machine

```{r}
#| warning: false

# Select features
features_diff <- c(
  "efg_diff", "tov_diff", "orb_diff", "ftr_diff",
  "ortg_diff", "drtg_diff", "net_diff", "pace_avg"
)

# Prepare data
train_diff <- train_data %>%
  select(score_diff, all_of(features_diff)) %>%
  drop_na()

test_diff <- test_data %>%
  select(score_diff, all_of(features_diff)) %>%
  drop_na()

# Train model
set.seed(42)
model_diff <- train(
  score_diff ~ .,
  data = train_diff,
  method = "gbm",
  trControl = trainControl(method = "cv", number = 5),
  verbose = FALSE,
  tuneLength = 3
)

# Evaluate
pred_diff <- predict(model_diff, test_diff)
test_diff$predicted <- pred_diff

# Mean Absolute Error
mae_diff <- mean(abs(test_diff$score_diff - test_diff$predicted))
cat("Test Set MAE:", round(mae_diff, 2), "points\n")

# Residuals plot
test_diff %>%
  ggplot(aes(x = predicted, y = score_diff)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "Model 2: Predicted vs Actual Score Differential",
    x = "Predicted Differential",
    y = "Actual Differential",
    caption = "Red line = perfect prediction"
  )
```

**Interpretation:** On average, predictions are off by about 10.55 points. That's the model's typical error.

---

## Model 3: Combined Score Prediction

**Goal:** Estimate the likely total combined score.

**Algorithm:** Random Forest (robust to outliers, handles interactions well)

```{r}
#| warning: false

# Select features
features_total <- c(
  "home_ortg", "away_ortg", "home_drtg", "away_drtg",
  "home_pace", "away_pace", "pace_avg"
)

# Prepare data
train_total <- train_data %>%
  select(total_score, all_of(features_total)) %>%
  drop_na()

test_total <- test_data %>%
  select(total_score, all_of(features_total)) %>%
  drop_na()

# Train model
set.seed(42)
model_total <- train(
  total_score ~ .,
  data = train_total,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  tuneLength = 3,
  ntree = 100
)

# Evaluate
pred_total <- predict(model_total, test_total)
test_total$predicted <- pred_total

# Mean Absolute Error
mae_total <- mean(abs(test_total$total_score - test_total$predicted))
cat("Test Set MAE:", round(mae_total, 2), "points\n")

# Residuals plot
test_total %>%
  ggplot(aes(x = predicted, y = total_score)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "Model 3: Predicted vs Actual Total Score",
    x = "Predicted Total",
    y = "Actual Total",
    caption = "Red line = perfect prediction"
  )
```

**Interpretation:** Total score predictions average about 13.7 points of error.

---


## Step 5: Apply Models to Kansas vs. Miami

Now let's use our trained models to analyze a hypothetical match-up.

```{r}
#| warning: false
#| message: false

# Get current season data
current_season <- load_mbb_team_box(seasons = 2026) %>% 
  clean_names() %>%
  mutate(season = 2026)

# Filter to Kansas and Miami
hawks_2026 <- current_season %>%
  filter(team_display_name %in% c("Miami (OH) RedHawks", "Kansas Jayhawks"))

# Calculate features (same process as training data)
hawks_features <- hawks_2026 %>%
  mutate(
    efg_pct = (field_goals_made + 0.5 * three_point_field_goals_made) / 
              field_goals_attempted,
    tov_pct = turnovers / 
              (field_goals_attempted + 0.44 * free_throws_attempted + turnovers),
    orb_pct = offensive_rebounds / 
              (offensive_rebounds + defensive_rebounds),
    ft_rate = free_throws_made / field_goals_attempted,
    possessions = field_goals_attempted - offensive_rebounds + 
                  turnovers + 0.44 * free_throws_attempted,
    ortg = (team_score / possessions) * 100,
    drtg = (opponent_team_score / possessions) * 100,
    net_rating = ortg - drtg,
    pace = possessions
  ) %>%
  arrange(team_id, game_date) %>%
  group_by(team_id) %>%
  mutate(
    efg_L5 = zoo::rollmean(efg_pct, k = 5, fill = NA, align = "right"),
    tov_L5 = zoo::rollmean(tov_pct, k = 5, fill = NA, align = "right"),
    orb_L5 = zoo::rollmean(orb_pct, k = 5, fill = NA, align = "right"),
    ftr_L5 = zoo::rollmean(ft_rate, k = 5, fill = NA, align = "right"),
    ortg_L5 = zoo::rollmean(ortg, k = 5, fill = NA, align = "right"),
    drtg_L5 = zoo::rollmean(drtg, k = 5, fill = NA, align = "right"),
    net_L5 = zoo::rollmean(net_rating, k = 5, fill = NA, align = "right"),
    pace_L5 = zoo::rollmean(pace, k = 5, fill = NA, align = "right")
  ) %>%
  ungroup()

# Get latest stats for each team
kansas_latest <- hawks_features %>%
  filter(team_display_name == "Kansas Jayhawks") %>%
  arrange(desc(game_date)) %>%
  slice(1)

miami_latest <- hawks_features %>%
  filter(team_display_name == "Miami (OH) RedHawks") %>%
  arrange(desc(game_date)) %>%
  slice(1)

# Create matchup data (assuming Kansas at home)
matchup <- tibble(
  home_team = "Kansas Jayhawks",
  away_team = "Miami (OH) RedHawks",
  home_efg = kansas_latest$efg_L5,
  home_tov = kansas_latest$tov_L5,
  home_orb = kansas_latest$orb_L5,
  home_ftr = kansas_latest$ftr_L5,
  home_ortg = kansas_latest$ortg_L5,
  home_drtg = kansas_latest$drtg_L5,
  home_net = kansas_latest$net_L5,
  home_pace = kansas_latest$pace_L5,
  away_efg = miami_latest$efg_L5,
  away_tov = miami_latest$tov_L5,
  away_orb = miami_latest$orb_L5,
  away_ftr = miami_latest$ftr_L5,
  away_ortg = miami_latest$ortg_L5,
  away_drtg = miami_latest$drtg_L5,
  away_net = miami_latest$net_L5,
  away_pace = miami_latest$pace_L5
) %>%
  mutate(
    efg_diff = home_efg - away_efg,
    tov_diff = home_tov - away_tov,
    orb_diff = home_orb - away_orb,
    ftr_diff = home_ftr - away_ftr,
    ortg_diff = home_ortg - away_ortg,
    drtg_diff = home_drtg - away_drtg,
    net_diff = home_net - away_net,
    pace_avg = (home_pace + away_pace) / 2
  )


# Generate predictions
pred_win_prob <- predict(model_binary, matchup, type = "prob")$Yes
pred_diff <- predict(model_diff, matchup)
pred_total <- predict(model_total, matchup)

# Display results
results <- tibble(
  Metric = c("Home Win Probability", "Expected Score Differential", "Expected Total Score"),
  Value = c(
    paste0(round(pred_win_prob * 100, 1), "%"),
    paste0(if_else(pred_diff > 0, "+", ""), round(pred_diff, 1), " points"),
    paste0(round(pred_total, 1), " points")
  )
)

results %>%
  knitr::kable(
    caption = "Kansas (Home) vs Miami: Model Predictions",
    align = c("l", "r")
  )
```

---

## Interpreting the Results

**Win Probability:** Based on recent form and match-up metrics, the model estimates likelihood of home team success. Given that my model tends to default to the home-team winning, this one is quite a shocker.

**Score Differential:** The expected margin. Positive means home team favored, negative means away team favored.

**Total Score:** The expected combined output from both teams. Influenced by pace and efficiency.

**Take-away:** I'm very surprised that, even when displaying results that are well within our established margin-of-error, we are seeing compelling evidence that a hypothetical game between Miami and Kansas at Allen Fieldhouse in Lawrence is essentially a toss-up/leans Miami.

The reason I say toss up is because the of the result of the margin model (expected score differential). That model showed an MAE of 10.55. I would not count on Miami actually beating the Jayhawks unless the prediction for the match-up resulted in a value of, say, -11. In which case, yeah I'd declare that I'm picking Miami.

---

## What Makes These Models Useful

**They're trained on real data:** Thousands of historical games with known outcomes.

**They use predictive features:** Rolling averages prevent overfitting to single-game variance.

**They're conservative:** MAE of 8-12 points means we know our uncertainty.

**They're explainable:** Coefficients and feature importance show what drives predictions.

---

## Feature Importance

Let's see what matters most for score total predictions:

```{r}
#| fig-width: 8
#| fig-height: 5

# Extract feature importance from Random Forest model
importance <- varImp(model_total, scale = FALSE)

# Plot
plot(importance, top = 8, main = "Top Features: Score Differential Model")
```

---

## Limitations and Honest Assessment

**What these models don't account for:**

- Injuries or lineup changes
- Motivational factors (rivalry games, tournament pressure)
- Referee tendencies
- Weather or travel fatigue
- Matchup-specific adjustments

**Why we're transparent about error rates:**

An MAE of 10 points on differential means roughly 2 out of 3 predictions fall within 10 points of actual. That's useful but not clairvoyant. We show our work so you can judge the quality yourself.

**How to use these predictions:**

They're inputs to your own analysis, not commandments. If our model says Team A is slightly favored but you know their starting point guard is injured, adjust accordingly. Statistical models provide baselines; domain knowledge provides context.

---

## Reproducibility

All code in this post is reproducible. You can run it yourself with:

1. Install packages: `hoopR`, `tidyverse`, `caret`, `janitor`
2. Copy the code chunks sequentially
3. Adjust team names to analyze any matchup

The data comes from ESPN via hoopR (free). The models are standard algorithms (no proprietary methods). The methodology is documented here.

---

## Conclusion

This concludes my three part series on building college basketball prediction models.

---

*Want to see these predictions daily? We apply this exact pipeline to generate analysis for premium subscribers. Methodology transparent. Results tracked. No black boxes.*
