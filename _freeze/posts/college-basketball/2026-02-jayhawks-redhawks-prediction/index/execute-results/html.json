{
  "hash": "87d50ae08ebb9883de6f3788c2776eb3",
  "result": {
    "markdown": "---\ntitle: \"A Tale of Two Hawks Part 3\"\ndescription: \"How to train, validate and deploy predictive models\"\ndate: last-modified\ncategories: [college-basketball, machine-learning, tutorial, kansas, miami-oh]\nauthor: \"ProPlotFits\"\n---\n\n\n## Introduction\n\nWhy am I trying to see who would win in a college basketball team between Kansas of the Big 12 and Miami of the MAC? These teams haven't played each other since 2011. All of those answers can be answered in Part 1 and Part 2.\n\nHere, the goal is to find out what would happen if Kansas and Miami were to play each other.\n\n---\n\n## The Three Prediction Tasks\n\nWe have three outcomes that require three models:\n\n**Model 1: Binary Classification** - Will one specific team win?  \n**Model 2: Continuous Regression (Difference)** - By how much might the score differ between teams?  \n**Model 3: Continuous Regression (Sum)** - What might the combined scoring output look like?\n\nEach model uses the same underlying features but optimizes for different objectives. Together they give us a complete picture of what might happen.\n\n---\n\n## Step 1: Prepare Historical Data\n\nWe need clean, structured data where each row represents one game from the perspective of both teams.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(hoopR)\nlibrary(janitor)\nlibrary(caret)\n\n# Load historical data (2024 and 2025 seasons)\nbox_scores_2024 <- load_mbb_team_box(seasons = 2024) %>% clean_names()\nbox_scores_2025 <- load_mbb_team_box(seasons = 2025) %>% clean_names()\n\n# Combine seasons\nall_games <- bind_rows(\n  box_scores_2024 %>% mutate(season = 2024),\n  box_scores_2025 %>% mutate(season = 2025)\n)\n\nglimpse(all_games)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 25,052\nColumns: 57\n$ game_id                           <int> 401638645, 401638645, 401638644, 401…\n$ season                            <dbl> 2024, 2024, 2024, 2024, 2024, 2024, …\n$ season_type                       <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ game_date                         <date> 2024-04-08, 2024-04-08, 2024-04-06,…\n$ game_date_time                    <dttm> 2024-04-08 21:20:00, 2024-04-08 21:…\n$ team_id                           <int> 2509, 41, 333, 41, 152, 2509, 282, 2…\n$ team_uid                          <chr> \"s:40~l:41~t:2509\", \"s:40~l:41~t:41\"…\n$ team_slug                         <chr> \"purdue-boilermakers\", \"uconn-huskie…\n$ team_location                     <chr> \"Purdue\", \"UConn\", \"Alabama\", \"UConn…\n$ team_name                         <chr> \"Boilermakers\", \"Huskies\", \"Crimson …\n$ team_abbreviation                 <chr> \"PUR\", \"CONN\", \"ALA\", \"CONN\", \"NCSU\"…\n$ team_display_name                 <chr> \"Purdue Boilermakers\", \"UConn Huskie…\n$ team_short_display_name           <chr> \"Purdue\", \"UConn\", \"Alabama\", \"UConn…\n$ team_color                        <chr> \"000000\", \"0c2340\", \"9e1632\", \"0c234…\n$ team_alternate_color              <chr> \"cfb991\", \"f1f2f3\", \"ffffff\", \"f1f2f…\n$ team_logo                         <chr> \"https://a.espncdn.com/i/teamlogos/n…\n$ team_home_away                    <chr> \"away\", \"home\", \"away\", \"home\", \"awa…\n$ team_score                        <int> 60, 75, 72, 86, 50, 63, 77, 79, 67, …\n$ team_winner                       <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRU…\n$ assists                           <int> 8, 18, 9, 20, 10, 13, 22, 12, 11, 18…\n$ blocks                            <int> 3, 4, 5, 8, 3, 2, 1, 6, 8, 6, 0, 4, …\n$ defensive_rebounds                <int> 19, 21, 21, 25, 22, 30, 29, 24, 24, …\n$ fast_break_points                 <chr> \"0\", \"2\", \"0\", \"2\", \"2\", \"0\", \"9\", \"…\n$ field_goal_pct                    <dbl> 44.4, 48.4, 44.8, 50.0, 36.8, 40.0, …\n$ field_goals_made                  <int> 24, 30, 26, 31, 21, 22, 28, 28, 25, …\n$ field_goals_attempted             <int> 54, 62, 58, 62, 57, 55, 60, 62, 65, …\n$ flagrant_fouls                    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fouls                             <int> 15, 18, 15, 17, 13, 8, 16, 17, 9, 14…\n$ free_throw_pct                    <dbl> 73.3, 81.8, 81.8, 77.8, 75.0, 90.0, …\n$ free_throws_made                  <int> 11, 9, 9, 14, 3, 9, 9, 17, 12, 12, 5…\n$ free_throws_attempted             <int> 15, 11, 11, 18, 4, 10, 10, 19, 17, 1…\n$ largest_lead                      <chr> \"2\", \"18\", \"5\", \"16\", \"0\", \"20\", \"7\"…\n$ offensive_rebounds                <int> 9, 14, 8, 12, 6, 11, 8, 7, 8, 13, 5,…\n$ points_in_paint                   <chr> \"40\", \"44\", \"26\", \"38\", \"20\", \"24\", …\n$ steals                            <int> 3, 3, 2, 4, 8, 5, 3, 8, 7, 9, 3, 5, …\n$ team_turnovers                    <int> 0, 2, 1, 0, 0, 2, 1, 0, 0, 2, 0, 0, …\n$ technical_fouls                   <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, …\n$ three_point_field_goal_pct        <dbl> 14.3, 27.3, 47.8, 40.0, 26.3, 40.0, …\n$ three_point_field_goals_made      <int> 1, 6, 11, 10, 5, 10, 12, 6, 5, 8, 17…\n$ three_point_field_goals_attempted <int> 7, 22, 23, 25, 19, 25, 32, 17, 26, 2…\n$ total_rebounds                    <int> 28, 35, 29, 37, 28, 41, 37, 31, 32, …\n$ total_technical_fouls             <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, …\n$ total_turnovers                   <int> 9, 8, 8, 4, 11, 16, 15, 8, 10, 11, 9…\n$ turnover_points                   <chr> \"13\", \"11\", \"6\", \"8\", \"10\", \"11\", \"1…\n$ turnovers                         <int> 9, 8, 8, 4, 11, 16, 15, 8, 10, 11, 9…\n$ opponent_team_id                  <int> 41, 2509, 41, 333, 2509, 152, 2550, …\n$ opponent_team_uid                 <chr> \"s:40~l:41~t:41\", \"s:40~l:41~t:2509\"…\n$ opponent_team_slug                <chr> \"uconn-huskies\", \"purdue-boilermaker…\n$ opponent_team_location            <chr> \"UConn\", \"Purdue\", \"UConn\", \"Alabama…\n$ opponent_team_name                <chr> \"Huskies\", \"Boilermakers\", \"Huskies\"…\n$ opponent_team_abbreviation        <chr> \"CONN\", \"PUR\", \"CONN\", \"ALA\", \"PUR\",…\n$ opponent_team_display_name        <chr> \"UConn Huskies\", \"Purdue Boilermaker…\n$ opponent_team_short_display_name  <chr> \"UConn\", \"Purdue\", \"UConn\", \"Alabama…\n$ opponent_team_color               <chr> \"0c2340\", \"000000\", \"0c2340\", \"9e163…\n$ opponent_team_alternate_color     <chr> \"f1f2f3\", \"cfb991\", \"f1f2f3\", \"fffff…\n$ opponent_team_logo                <chr> \"https://a.espncdn.com/i/teamlogos/n…\n$ opponent_team_score               <int> 75, 60, 86, 72, 63, 50, 79, 77, 84, …\n```\n:::\n:::\n\n\n---\n\n## Step 2: Feature Engineering\n\nWe calculate team-level statistics that capture performance quality.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Calculate four factors and efficiency metrics\nteam_features <- all_games %>%\n  mutate(\n    # Four Factors\n    efg_pct = (field_goals_made + 0.5 * three_point_field_goals_made) / \n              field_goals_attempted,\n    tov_pct = turnovers / \n              (field_goals_attempted + 0.44 * free_throws_attempted + turnovers),\n    orb_pct = offensive_rebounds / \n              (offensive_rebounds + defensive_rebounds),\n    ft_rate = free_throws_made / field_goals_attempted,\n    \n    # Efficiency\n    possessions = field_goals_attempted - offensive_rebounds + \n                  turnovers + 0.44 * free_throws_attempted,\n    ortg = (team_score / possessions) * 100,\n    drtg = (opponent_team_score / possessions) * 100,\n    net_rating = ortg - drtg,\n    pace = possessions\n  ) %>%\n  select(\n    season, game_date, game_id,\n    team_display_name, team_id, team_home_away,\n    team_score, opponent_team_score,\n    efg_pct, tov_pct, orb_pct, ft_rate,\n    ortg, drtg, net_rating, pace\n  )\n\n# Calculate rolling averages (last 5 games)\nteam_rolling <- team_features %>%\n  arrange(team_id, game_date) %>%\n  group_by(team_id) %>%\n  mutate(\n    efg_L5 = lag(zoo::rollmean(efg_pct, k = 5, fill = NA, align = \"right\")),\n    tov_L5 = lag(zoo::rollmean(tov_pct, k = 5, fill = NA, align = \"right\")),\n    orb_L5 = lag(zoo::rollmean(orb_pct, k = 5, fill = NA, align = \"right\")),\n    ftr_L5 = lag(zoo::rollmean(ft_rate, k = 5, fill = NA, align = \"right\")),\n    ortg_L5 = lag(zoo::rollmean(ortg, k = 5, fill = NA, align = \"right\")),\n    drtg_L5 = lag(zoo::rollmean(drtg, k = 5, fill = NA, align = \"right\")),\n    net_L5 = lag(zoo::rollmean(net_rating, k = 5, fill = NA, align = \"right\")),\n    pace_L5 = lag(zoo::rollmean(pace, k = 5, fill = NA, align = \"right\"))\n  ) %>%\n  ungroup() %>%\n  drop_na(efg_L5)  # Only keep games where we have rolling history\n\nhead(team_rolling)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 24\n  season game_date   game_id team_display_name team_id team_home_away team_score\n   <dbl> <date>        <int> <chr>               <int> <chr>               <int>\n1   2024 2023-11-29   4.02e8 Auburn Tigers           2 home                   74\n2   2024 2023-12-03   4.02e8 Auburn Tigers           2 away                   64\n3   2024 2023-12-09   4.02e8 Auburn Tigers           2 away                  104\n4   2024 2023-12-13   4.02e8 Auburn Tigers           2 home                   87\n5   2024 2023-12-17   4.02e8 Auburn Tigers           2 home                   91\n6   2024 2023-12-22   4.02e8 Auburn Tigers           2 home                   82\n# ℹ 17 more variables: opponent_team_score <int>, efg_pct <dbl>, tov_pct <dbl>,\n#   orb_pct <dbl>, ft_rate <dbl>, ortg <dbl>, drtg <dbl>, net_rating <dbl>,\n#   pace <dbl>, efg_L5 <dbl>, tov_L5 <dbl>, orb_L5 <dbl>, ftr_L5 <dbl>,\n#   ortg_L5 <dbl>, drtg_L5 <dbl>, net_L5 <dbl>, pace_L5 <dbl>\n```\n:::\n:::\n\n\n**Key concept:** We use lagged rolling averages so we only use information available *before* each game. This prevents data leakage.\n\n---\n\n## Step 3: Adjust Data Granularity\n\nModels predict games, not individual team performances. We need to pivot from team-game level to game level.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Separate home and away\nhome_games <- team_rolling %>%\n  filter(team_home_away == \"home\") %>%\n  select(\n    game_id, season, game_date,\n    home_team = team_display_name,\n    home_score = team_score,\n    home_efg = efg_L5, home_tov = tov_L5, home_orb = orb_L5, home_ftr = ftr_L5,\n    home_ortg = ortg_L5, home_drtg = drtg_L5, home_net = net_L5, home_pace = pace_L5\n  )\n\naway_games <- team_rolling %>%\n  filter(team_home_away == \"away\") %>%\n  select(\n    game_id,\n    away_team = team_display_name,\n    away_score = team_score,\n    away_efg = efg_L5, away_tov = tov_L5, away_orb = orb_L5, away_ftr = ftr_L5,\n    away_ortg = ortg_L5, away_drtg = drtg_L5, away_net = net_L5, away_pace = pace_L5\n  )\n\n# Join and calculate outcomes\ngames <- home_games %>%\n  inner_join(away_games, by = \"game_id\") %>%\n  mutate(\n    # Outcome variables\n    home_win = as.factor(if_else(home_score > away_score, \"Yes\", \"No\")),\n    score_diff = home_score - away_score,  # positive = home won\n    total_score = home_score + away_score,\n    \n    # Matchup differentials\n    efg_diff = home_efg - away_efg,\n    tov_diff = home_tov - away_tov,\n    orb_diff = home_orb - away_orb,\n    ftr_diff = home_ftr - away_ftr,\n    ortg_diff = home_ortg - away_ortg,\n    drtg_diff = home_drtg - away_drtg,\n    net_diff = home_net - away_net,\n    pace_avg = (home_pace + away_pace) / 2\n  ) %>%\n  drop_na()\n\nglimpse(games)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 10,706\nColumns: 34\n$ game_id     <int> 401574556, 401583797, 401583798, 401583799, 401583800, 401…\n$ season      <dbl> 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024…\n$ game_date   <date> 2023-11-29, 2023-12-13, 2023-12-17, 2023-12-22, 2023-12-3…\n$ home_team   <chr> \"Auburn Tigers\", \"Auburn Tigers\", \"Auburn Tigers\", \"Auburn…\n$ home_score  <int> 74, 87, 91, 82, 101, 88, 66, 93, 82, 81, 99, 101, 59, 78, …\n$ home_efg    <dbl> 0.5411528, 0.4976382, 0.5032872, 0.5060294, 0.5290879, 0.5…\n$ home_tov    <dbl> 0.15635339, 0.11422651, 0.09844881, 0.08669590, 0.10987814…\n$ home_orb    <dbl> 0.3092027, 0.3436161, 0.3289103, 0.3318864, 0.3005450, 0.2…\n$ home_ftr    <dbl> 0.3115414, 0.3278700, 0.2922931, 0.2797149, 0.2985918, 0.3…\n$ home_ortg   <dbl> 115.9966, 116.5411, 118.5297, 119.7185, 120.0627, 129.3485…\n$ home_drtg   <dbl> 93.00273, 91.96987, 91.83074, 97.11057, 97.06614, 94.68838…\n$ home_net    <dbl> 22.993876, 24.571234, 26.698965, 22.607937, 22.996599, 34.…\n$ home_pace   <dbl> 71.112, 68.808, 69.352, 69.800, 71.048, 72.024, 70.832, 69…\n$ away_team   <chr> \"Virginia Tech Hokies\", \"UNC Asheville Bulldogs\", \"USC Tro…\n$ away_score  <int> 57, 62, 75, 62, 66, 68, 55, 78, 59, 54, 81, 61, 70, 63, 78…\n$ away_efg    <dbl> 0.4951475, 0.4959706, 0.5466798, 0.4568202, 0.5823009, 0.5…\n$ away_tov    <dbl> 0.1462222, 0.1420995, 0.1595115, 0.1317813, 0.1235595, 0.1…\n$ away_orb    <dbl> 0.2392959, 0.2960297, 0.3057633, 0.3065729, 0.2551598, 0.2…\n$ away_ftr    <dbl> 0.3091771, 0.3705427, 0.2814960, 0.1790102, 0.2515047, 0.1…\n$ away_ortg   <dbl> 105.03712, 108.30775, 111.93998, 99.49453, 122.49550, 110.…\n$ away_drtg   <dbl> 98.71165, 97.90928, 107.47446, 98.58639, 103.04818, 107.07…\n$ away_net    <dbl> 6.3254647, 10.3984684, 4.4655215, 0.9081459, 19.4473169, 3…\n$ away_pace   <dbl> 68.552, 73.464, 71.960, 70.416, 70.336, 67.512, 67.024, 73…\n$ home_win    <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes…\n$ score_diff  <int> 17, 25, 16, 20, 35, 20, 11, 15, 23, 27, 18, 40, -11, 15, 1…\n$ total_score <int> 131, 149, 166, 144, 167, 156, 121, 171, 141, 135, 180, 162…\n$ efg_diff    <dbl> 0.046005368, 0.001667557, -0.043392585, 0.049209190, -0.05…\n$ tov_diff    <dbl> 0.0101312371, -0.0278730341, -0.0610627094, -0.0450853666,…\n$ orb_diff    <dbl> 0.069906866, 0.047586434, 0.023146954, 0.025313589, 0.0453…\n$ ftr_diff    <dbl> 0.002364308, -0.042672706, 0.010797117, 0.100704608, 0.047…\n$ ortg_diff   <dbl> 10.9594823, 8.2333595, 6.5897241, 20.2239733, -2.4327520, …\n$ drtg_diff   <dbl> -5.708929, -5.939406, -15.643720, -1.475818, -5.982034, -1…\n$ net_diff    <dbl> 16.6684115, 14.1727659, 22.2334439, 21.6997914, 3.5492819,…\n$ pace_avg    <dbl> 69.832, 71.136, 70.656, 70.108, 70.692, 69.768, 68.928, 71…\n```\n:::\n:::\n\n\nNow each row is one game with features for both teams and three outcome variables.\n\n---\n\n## Step 4: Train-Test Split\n\nWe use 2024 season for training and 2025 season for testing. This simulates real-world usage: train on history, predict the future.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data <- games %>% filter(season == 2024)\ntest_data <- games %>% filter(season == 2025)\n\ncat(\"Training games:\", nrow(train_data), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining games: 4889 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Testing games:\", nrow(test_data), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTesting games: 5817 \n```\n:::\n:::\n\n\n---\n\n## Model 1: Binary Outcome Prediction\n\n**Goal:** Estimate probability that the home team wins\n\n**Algorithm:** Logistic Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select features for classification\nfeatures_binary <- c(\n  \"efg_diff\", \"tov_diff\", \"orb_diff\", \"ftr_diff\",\n  \"net_diff\", \"pace_avg\"\n)\n\n# Prepare data\ntrain_binary <- train_data %>%\n  select(home_win, all_of(features_binary)) %>%\n  drop_na()\n\ntest_binary <- test_data %>%\n  select(home_win, all_of(features_binary)) %>%\n  drop_na()\n\n# Train model\nset.seed(42)\nmodel_binary <- train(\n  home_win ~ .,\n  data = train_binary,\n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(\n    method = \"cv\",\n    number = 5,\n    classProbs = TRUE,\n    summaryFunction = twoClassSummary\n  ),\n  metric = \"ROC\"\n)\n\n# Evaluate\npred_binary <- predict(model_binary, test_binary, type = \"prob\")\ntest_binary$pred_prob <- pred_binary$Yes\ntest_binary$pred_class <- predict(model_binary, test_binary)\n\n# Accuracy\naccuracy <- mean(test_binary$pred_class == test_binary$home_win)\ncat(\"Test Set Accuracy:\", round(accuracy * 100, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Set Accuracy: 68.4 %\n```\n:::\n\n```{.r .cell-code}\n# Confusion Matrix\nconfusionMatrix(test_binary$pred_class, test_binary$home_win)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No   741  524\n       Yes 1315 3237\n                                          \n               Accuracy : 0.6839          \n                 95% CI : (0.6717, 0.6958)\n    No Information Rate : 0.6466          \n    P-Value [Acc > NIR] : 1.062e-09       \n                                          \n                  Kappa : 0.2422          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.3604          \n            Specificity : 0.8607          \n         Pos Pred Value : 0.5858          \n         Neg Pred Value : 0.7111          \n             Prevalence : 0.3534          \n         Detection Rate : 0.1274          \n   Detection Prevalence : 0.2175          \n      Balanced Accuracy : 0.6105          \n                                          \n       'Positive' Class : No              \n                                          \n```\n:::\n:::\n\n\n**Interpretation:** I'm really happy about this output, because it should spur us to investigate our data more. \n\nThe model predicted 1,315 false positives (it predicted 1,315 home wins that were actually losses) according to the [Confusion Matrix](https://www.youtube.com/watch?v=Kdsp6soqA7o). The model is likely over-relying on the home-court advantage, which can work slightly better than a coin flip but it is not nuanced enough to when the away team is actually better. \n\nUltimately, if I were to develop a machine learning model to help me determine moneyline, I would go for one that outputs probabilities rather than straight-up Yes/No outcomes. \n\n---\n\n## Model 2: Score Differential Prediction\n\n**Goal:** What is the likely difference in final scores?\n\n**Algorithm:** Gradient Boosting Machine\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select features\nfeatures_diff <- c(\n  \"efg_diff\", \"tov_diff\", \"orb_diff\", \"ftr_diff\",\n  \"ortg_diff\", \"drtg_diff\", \"net_diff\", \"pace_avg\"\n)\n\n# Prepare data\ntrain_diff <- train_data %>%\n  select(score_diff, all_of(features_diff)) %>%\n  drop_na()\n\ntest_diff <- test_data %>%\n  select(score_diff, all_of(features_diff)) %>%\n  drop_na()\n\n# Train model\nset.seed(42)\nmodel_diff <- train(\n  score_diff ~ .,\n  data = train_diff,\n  method = \"gbm\",\n  trControl = trainControl(method = \"cv\", number = 5),\n  verbose = FALSE,\n  tuneLength = 3\n)\n\n# Evaluate\npred_diff <- predict(model_diff, test_diff)\ntest_diff$predicted <- pred_diff\n\n# Mean Absolute Error\nmae_diff <- mean(abs(test_diff$score_diff - test_diff$predicted))\ncat(\"Test Set MAE:\", round(mae_diff, 2), \"points\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Set MAE: 10.55 points\n```\n:::\n\n```{.r .cell-code}\n# Residuals plot\ntest_diff %>%\n  ggplot(aes(x = predicted, y = score_diff)) +\n  geom_point(alpha = 0.4) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    title = \"Model 2: Predicted vs Actual Score Differential\",\n    x = \"Predicted Differential\",\n    y = \"Actual Differential\",\n    caption = \"Red line = perfect prediction\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n**Interpretation:** On average, predictions are off by about 10.55 points. That's the model's typical error.\n\n---\n\n## Model 3: Combined Score Prediction\n\n**Goal:** Estimate the likely total combined score.\n\n**Algorithm:** Random Forest (robust to outliers, handles interactions well)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select features\nfeatures_total <- c(\n  \"home_ortg\", \"away_ortg\", \"home_drtg\", \"away_drtg\",\n  \"home_pace\", \"away_pace\", \"pace_avg\"\n)\n\n# Prepare data\ntrain_total <- train_data %>%\n  select(total_score, all_of(features_total)) %>%\n  drop_na()\n\ntest_total <- test_data %>%\n  select(total_score, all_of(features_total)) %>%\n  drop_na()\n\n# Train model\nset.seed(42)\nmodel_total <- train(\n  total_score ~ .,\n  data = train_total,\n  method = \"rf\",\n  trControl = trainControl(method = \"cv\", number = 5),\n  tuneLength = 3,\n  ntree = 100\n)\n\n# Evaluate\npred_total <- predict(model_total, test_total)\ntest_total$predicted <- pred_total\n\n# Mean Absolute Error\nmae_total <- mean(abs(test_total$total_score - test_total$predicted))\ncat(\"Test Set MAE:\", round(mae_total, 2), \"points\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Set MAE: 13.71 points\n```\n:::\n\n```{.r .cell-code}\n# Residuals plot\ntest_total %>%\n  ggplot(aes(x = predicted, y = total_score)) +\n  geom_point(alpha = 0.4) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    title = \"Model 3: Predicted vs Actual Total Score\",\n    x = \"Predicted Total\",\n    y = \"Actual Total\",\n    caption = \"Red line = perfect prediction\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n**Interpretation:** Total score predictions average about 13.7 points of error.\n\n---\n\n\n## Step 5: Apply Models to Kansas vs. Miami\n\nNow let's use our trained models to analyze a hypothetical match-up.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get current season data\ncurrent_season <- load_mbb_team_box(seasons = 2026) %>% \n  clean_names() %>%\n  mutate(season = 2026)\n\n# Filter to Kansas and Miami\nhawks_2026 <- current_season %>%\n  filter(team_display_name %in% c(\"Miami (OH) RedHawks\", \"Kansas Jayhawks\"))\n\n# Calculate features (same process as training data)\nhawks_features <- hawks_2026 %>%\n  mutate(\n    efg_pct = (field_goals_made + 0.5 * three_point_field_goals_made) / \n              field_goals_attempted,\n    tov_pct = turnovers / \n              (field_goals_attempted + 0.44 * free_throws_attempted + turnovers),\n    orb_pct = offensive_rebounds / \n              (offensive_rebounds + defensive_rebounds),\n    ft_rate = free_throws_made / field_goals_attempted,\n    possessions = field_goals_attempted - offensive_rebounds + \n                  turnovers + 0.44 * free_throws_attempted,\n    ortg = (team_score / possessions) * 100,\n    drtg = (opponent_team_score / possessions) * 100,\n    net_rating = ortg - drtg,\n    pace = possessions\n  ) %>%\n  arrange(team_id, game_date) %>%\n  group_by(team_id) %>%\n  mutate(\n    efg_L5 = zoo::rollmean(efg_pct, k = 5, fill = NA, align = \"right\"),\n    tov_L5 = zoo::rollmean(tov_pct, k = 5, fill = NA, align = \"right\"),\n    orb_L5 = zoo::rollmean(orb_pct, k = 5, fill = NA, align = \"right\"),\n    ftr_L5 = zoo::rollmean(ft_rate, k = 5, fill = NA, align = \"right\"),\n    ortg_L5 = zoo::rollmean(ortg, k = 5, fill = NA, align = \"right\"),\n    drtg_L5 = zoo::rollmean(drtg, k = 5, fill = NA, align = \"right\"),\n    net_L5 = zoo::rollmean(net_rating, k = 5, fill = NA, align = \"right\"),\n    pace_L5 = zoo::rollmean(pace, k = 5, fill = NA, align = \"right\")\n  ) %>%\n  ungroup()\n\n# Get latest stats for each team\nkansas_latest <- hawks_features %>%\n  filter(team_display_name == \"Kansas Jayhawks\") %>%\n  arrange(desc(game_date)) %>%\n  slice(1)\n\nmiami_latest <- hawks_features %>%\n  filter(team_display_name == \"Miami (OH) RedHawks\") %>%\n  arrange(desc(game_date)) %>%\n  slice(1)\n\n# Create matchup data (assuming Kansas at home)\nmatchup <- tibble(\n  home_team = \"Kansas Jayhawks\",\n  away_team = \"Miami (OH) RedHawks\",\n  home_efg = kansas_latest$efg_L5,\n  home_tov = kansas_latest$tov_L5,\n  home_orb = kansas_latest$orb_L5,\n  home_ftr = kansas_latest$ftr_L5,\n  home_ortg = kansas_latest$ortg_L5,\n  home_drtg = kansas_latest$drtg_L5,\n  home_net = kansas_latest$net_L5,\n  home_pace = kansas_latest$pace_L5,\n  away_efg = miami_latest$efg_L5,\n  away_tov = miami_latest$tov_L5,\n  away_orb = miami_latest$orb_L5,\n  away_ftr = miami_latest$ftr_L5,\n  away_ortg = miami_latest$ortg_L5,\n  away_drtg = miami_latest$drtg_L5,\n  away_net = miami_latest$net_L5,\n  away_pace = miami_latest$pace_L5\n) %>%\n  mutate(\n    efg_diff = home_efg - away_efg,\n    tov_diff = home_tov - away_tov,\n    orb_diff = home_orb - away_orb,\n    ftr_diff = home_ftr - away_ftr,\n    ortg_diff = home_ortg - away_ortg,\n    drtg_diff = home_drtg - away_drtg,\n    net_diff = home_net - away_net,\n    pace_avg = (home_pace + away_pace) / 2\n  )\n\n\n# Generate predictions\npred_win_prob <- predict(model_binary, matchup, type = \"prob\")$Yes\npred_diff <- predict(model_diff, matchup)\npred_total <- predict(model_total, matchup)\n\n# Display results\nresults <- tibble(\n  Metric = c(\"Home Win Probability\", \"Expected Score Differential\", \"Expected Total Score\"),\n  Value = c(\n    paste0(round(pred_win_prob * 100, 1), \"%\"),\n    paste0(if_else(pred_diff > 0, \"+\", \"\"), round(pred_diff, 1), \" points\"),\n    paste0(round(pred_total, 1), \" points\")\n  )\n)\n\nresults %>%\n  knitr::kable(\n    caption = \"Kansas (Home) vs Miami: Model Predictions\",\n    align = c(\"l\", \"r\")\n  )\n```\n\n::: {.cell-output-display}\nTable: Kansas (Home) vs Miami: Model Predictions\n\n|Metric                      |       Value|\n|:---------------------------|-----------:|\n|Home Win Probability        |       43.4%|\n|Expected Score Differential | -2.4 points|\n|Expected Total Score        |  151 points|\n:::\n:::\n\n\n---\n\n## Interpreting the Results\n\n**Win Probability:** Based on recent form and match-up metrics, the model estimates likelihood of home team success. Given that my model tends to default to the home-team winning, this one is quite a shocker.\n\n**Score Differential:** The expected margin. Positive means home team favored, negative means away team favored.\n\n**Total Score:** The expected combined output from both teams. Influenced by pace and efficiency.\n\n**Take-away:** I'm very surprised that, even when displaying results that are well within our established margin-of-error, we are seeing compelling evidence that a hypothetical game between Miami and Kansas at Allen Fieldhouse in Lawrence is essentially a toss-up/leans Miami.\n\nThe reason I say toss up is because the of the result of the margin model (expected score differential). That model showed an MAE of 10.55. I would not count on Miami actually beating the Jayhawks unless the prediction for the match-up resulted in a value of, say, -11. In which case, yeah I'd declare that I'm picking Miami.\n\n---\n\n## What Makes These Models Useful\n\n**They're trained on real data:** Thousands of historical games with known outcomes.\n\n**They use predictive features:** Rolling averages prevent overfitting to single-game variance.\n\n**They're conservative:** MAE of 8-12 points means we know our uncertainty.\n\n**They're explainable:** Coefficients and feature importance show what drives predictions.\n\n---\n\n## Feature Importance\n\nLet's see what matters most for score total predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract feature importance from Random Forest model\nimportance <- varImp(model_total, scale = FALSE)\n\n# Plot\nplot(importance, top = 8, main = \"Top Features: Score Differential Model\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=768}\n:::\n:::\n\n\n---\n\n## Limitations and Honest Assessment\n\n**What these models don't account for:**\n\n- Injuries or lineup changes\n- Motivational factors (rivalry games, tournament pressure)\n- Referee tendencies\n- Weather or travel fatigue\n- Matchup-specific adjustments\n\n**Why we're transparent about error rates:**\n\nAn MAE of 10 points on differential means roughly 2 out of 3 predictions fall within 10 points of actual. That's useful but not clairvoyant. We show our work so you can judge the quality yourself.\n\n**How to use these predictions:**\n\nThey're inputs to your own analysis, not commandments. If our model says Team A is slightly favored but you know their starting point guard is injured, adjust accordingly. Statistical models provide baselines; domain knowledge provides context.\n\n---\n\n## Reproducibility\n\nAll code in this post is reproducible. You can run it yourself with:\n\n1. Install packages: `hoopR`, `tidyverse`, `caret`, `janitor`\n2. Copy the code chunks sequentially\n3. Adjust team names to analyze any matchup\n\nThe data comes from ESPN via hoopR (free). The models are standard algorithms (no proprietary methods). The methodology is documented here.\n\n---\n\n## Conclusion\n\nThis concludes my three part series on building college basketball prediction models.\n\n---\n\n*Want to see these predictions daily? We apply this exact pipeline to generate analysis for premium subscribers. Methodology transparent. Results tracked. No black boxes.*\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}